---
title:          "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding"
date:           2024-12-14 00:00:00 +0800
selected:       true
pub:            "Project co-lead and core contributor. "
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"

abstract: >-
  <details class="pub-abstract">
    <summary>Abstract</summary>
    <div class="abstract-content" style="margin-top:">
    DeepSeek-VL2 is a large multimodal foundation model based on the Mixture-of-Experts (MoE) architecture. It possesses a wide range of multimodal understanding capabilities, including image description, landmark recognition, chart understanding, OCR, meme understanding, multi-image understanding, object localization, and reasoning. Thanks to its MoE architecture, the model achieves better overall performance than Qwen2-VL-7B and InternVL2-8B while using only 4.1B active parameters. In terms of visual perception (specifically image description and vision perception), it surpasses Qwen2-VL-72B.
    </div>
  </details>

cover: /_publications/2024/dsvl2.png
authors:
  - DeepSeek
links:
  Paper: https://arxiv.org/abs/2412.10302
  Code: https://github.com/deepseek-ai/DeepSeek-VL2
  官方介绍: https://mp.weixin.qq.com/s/rE6Dh_OzolgDTAh3ubM5KA
---
