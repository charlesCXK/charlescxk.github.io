---
title:          "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"
date:           2023-6-14 00:00:00 +0800
selected:       true
pub:            "Neural Information Processing Systems (NeurIPS)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2023"

abstract: >-
  <details class="pub-abstract">
    <summary>Abstract</summary>
    <div class="abstract-content" style="margin-top:">
    We propose a vision-centric task framework based on large language models (LLMs). By treating images as a form of language and aligning vision tasks with language tasks—which can be flexibly defined and managed through linguistic instructions—this framework provides a unified perspective for both vision and language tasks. VisionLLM enables task customization at various levels via language instructions, ranging from fine-grained object-level to coarse-grained task-level customization. It achieves over 60% mAP on COCO, comparable to specialized detection models.
    </div>
  </details>

cover: /_publications/2023/VisionLLM.png
authors:
  - Wenhai Wang*, 
  - Zhe Chen*
  - Xiaokang Chen*
  - Jiannan Wu*
  - Xizhou Zhu
  - Gang Zeng
  - Ping Luo
  - Tong Lu
  - Jie Zhou
  - Yu Qiao
  - Jifeng Dai
links:
  Paper: https://arxiv.org/pdf/2305.11175.pdf
  Code: https://github.com/OpenGVLab/VisionLLM
  Demo: https://github.com/OpenGVLab/InternGPT
---
